# Kâ€‘Means Image Segmentation & Compression (from scratch)
Adaptive Color Quantization for Image Compression

> A clear, representation of **kâ€‘means clustering** for image segmentation and color compression. Pixels are clustered in RGB space; each pixel is replaced by its clusterâ€™s color to reconstruct a compressed image. Includes two init strategies (random, kâ€‘means++â€‘style spaced), multiple **k** values, and quantitative evaluation with **MSE**.

---

## ğŸ“¸ Quick look â€” inputs & outputs (placeholders)

<div align="center">
  <img src="https://github.com/user-attachments/assets/5a3ac8fa-85d0-4339-8e5f-ba81ec9f65cb" width="753" height="1180" alt="Original image â€“ cat"/>
  <br/>
  <sub><b>Fig 1.</sub>

<div align="center">
  <img src="https://github.com/user-attachments/assets/6253f4ad-f406-47f0-873c-68951fdee499" width="753" height="1180" alt="Original image â€“ car"/>
  <br/>
  <sub><b>Fig 2.</sub>
</div>


---

ğŸ§  Plainâ€‘English idea

An image is a grid of pixels; each pixel has an RGB color like a point in 3â€‘D space. kâ€‘means groups similar colors together into k clusters. After clustering, every pixel is painted with its clusterâ€™s average color. The result uses only k colors, which:

Segments the image into coherent color regions (sky vs. leaves vs. fur), and

Compresses the color palette (fewer distinct colors â†’ smaller representation).

This is useful when a simpler, flatter color representation is enough (posters, icons, previews) or as a preprocessing step for other vision tasks.

âœï¸ What the code does (at a glance)

Loads two images (e.g., cat.jpg, car.jpg).

Flattens each image to a matrix X of shape (num_pixels, 3) with one RGB row per pixel.

Runs kâ€‘means for multiple k values: [2, 3, 10, 20, 40].

For each k, runs three initializations: random (twice) and spaced (kâ€‘means++â€‘style).

Reconstructs the compressed image and saves it to results/â€¦.

Computes MSE between original and reconstruction and logs it in results.txt.

Key functions (see code): initialize_centers_random, initialize_centers_spaced (kâ€‘means++â€‘inspired), kmeans, reconstruct_image, compute_mse.

ğŸ”¬ From intuition to math

Objective (what kâ€‘means is trying to minimize)

Given data points (here, pixel colors) , choose  centroids  and assignments so that points are close to their clusterâ€™s centroid. The withinâ€‘cluster sum of squares (WCSS) is



where  is the set of points assigned to cluster , and  is the Euclidean norm. Smaller WCSS means tighter, more coherent clusters.

The two steps that repeat (Lloydâ€™s algorithm)

Assign each pixel to its nearest centroid:



Update each centroid to the mean of its assigned points:



Repeat until centroids stop moving (or a max number of iterations is reached). Each repeat never increases WCSS, so the process converges to a local minimum.

Why initialization matters

kâ€‘means can settle in different local minima depending on the starting centroids.

Random init: pick  random pixels.

Spaced init (kâ€‘means++â€‘style): choose the first pixel randomly; then probabilistically pick new centroids far from those already chosen (proportional to squared distance). This spreads initial centers and often speeds convergence / improves quality.

ğŸ§© Reconstructing and evaluating the image

After clustering, rebuild the image by replacing each pixel by its cluster mean color:



To quantify fidelity, compute Mean Squared Error (MSE) between the original image  and the reconstruction :



where . Lower MSE  better reconstruction (but human perception doesnâ€™t always align perfectly with MSE).

ğŸ› ï¸ Implementation details (mapping to the code)

Data prep: Image.open(...).convert('RGB') â†’ np.array(...) â†’ reshape to (num_pixels, 3).

Init strategies:

initialize_centers_random(X, k): uniform random sample of k pixels.

initialize_centers_spaced(X, k): first center random; subsequent centers sampled with probability  where  is distance to the nearest chosen center (kâ€‘means++ logic).

Main loop: kmeans(X, k, init_strategy, max_iters=100) repeatedly assigns labels and recomputes centroids until convergence.

Outputs: reconstruct_image(labels, centers, image_shape) â†’ save PNG; compute_mse(...) â†’ log to results.txt along with iteration count.

Batching: the code uses vectorized NumPy operations for distances and assignment for speed.

Tip: results are written under results/image_1/ and results/image_2/, one PNG per (k, init) plus a results.txt summary.

ğŸ“ˆ What to expect as k changes

Small k (e.g., 2â€“3): strong â€œposterizationâ€ (large, flat color regions). Compression is high; detail is lost. MSE is usually higher.

Moderate k (e.g., 10â€“20): preserves large structures and many textures; good tradeâ€‘off. MSE drops notably.

Large k (e.g., 40+): close to the original; diminishing returns in MSE vs. added complexity.

Initialization impacts quality/time mildly. Spaced init often converges in fewer iterations and can reduce MSE, but the best choice depends on image content.

ğŸ“Œ Results gallery (placeholders)

Use a few representative reconstructions to keep the README readable. Suggested layout:

Image 1 (cat)

Image 2 (car)

Swap in your uploaded images from results/.... Keep widths modest (â‰¤500) so the page stays readable.

ğŸ¤” Common questions (brief)

Why RGB space? Itâ€™s simple and works well for colorâ€‘driven segmentation. Alternatives like Lab/HSV can sometimes cluster perceptually better.

Will kâ€‘means always find the best clustering? Noâ€”only a local optimum. Thatâ€™s why initialization (and multiple restarts) can help.

Is MSE the best metric? Itâ€™s convenient and standard, but perceptual metrics (SSIM) may correlate better with human judgment.

ğŸ§­ Extensions

kâ€‘means++ exactly: use the canonical seeding (this code already mirrors the idea).

Color spaces: run in Lab for perceptual uniformity.

Spatial regularization: add pixel position () to the feature vector to discourage speckle.

Autoâ€‘choose k: elbow method or information criteria.

ğŸ—‚ï¸ Repository pointers

SL4_A3 (2).py â€” full implementation (init strategies, kâ€‘means loop, reconstruction, MSE logging).

SL4_A3_Report.pdf â€” writeâ€‘up with theory, experiments, and discussion.

License

MIT â€” see LICENSE.



